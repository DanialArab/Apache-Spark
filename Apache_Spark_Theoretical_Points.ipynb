{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7704f75a",
   "metadata": {},
   "source": [
    "# Course notes and points \n",
    "(this notebook contains all the theories and my notes from the class and all the coding and assignments are in the other notebook in my ubuntu VM called \"class_coding\", so later I need to push these two files into my github as a spark portfolio)\n",
    "\n",
    "sources: \n",
    "Spark and Python for Big Data class by Jose Portilla on udemy\n",
    "\n",
    "https://spark.apache.org/docs/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259cae4",
   "metadata": {},
   "source": [
    "# Apache Spark vs. Pandas \n",
    "\n",
    "source: https://www.youtube.com/watch?v=XrpSRCwISdk\n",
    "\n",
    "Apache Spark is an open-source distributed computing system designed for processing large datasets. It runs on distributed compute like YARN, Mesos, Standalone cluster.\n",
    "\n",
    "PySpark is the Python API for Apache Spark.\n",
    "\n",
    "Apache Spark has two main abstractions: \n",
    "+ RDD - distributed collection of objects\n",
    "+ Dataframe - distributed dataset of tabular data.\n",
    "  + Integrated SQL\n",
    "  + ML Algorithms \n",
    "\n",
    "Gain and lose of using PySpark over Pandas:\n",
    "\n",
    "Gain:\n",
    "+ Ability to work with big data\n",
    "+ Native SQL support \n",
    "+ Decent documentation\n",
    "\n",
    "Lose:\n",
    "+ Amazing documentation\n",
    "+ easy plotting\n",
    "+ indices\n",
    "\n",
    "Pandas:\n",
    "  + df = pd.read_csv (path_to_csv_file) \n",
    "  + df\n",
    "  + df.head (10)\n",
    "  + df.columns \n",
    "  + df.dtypes\n",
    "  + df.columns = ['a', 'b'] \n",
    "  + df.rename(columns = {'old': 'new'}) \n",
    "  + df.drop ('mpg', axis = 1) \n",
    "  + df[df.mpg < 20] \n",
    "  + df['gpm'] = 1 / df.mpg\n",
    "  + df.fillna(0)\n",
    "  + df.groupby(['cyl', 'gear']).agg({'mpg': 'mean', 'disp': 'min'}) \n",
    "  + import numpy as np\n",
    "    + df['logdisp'] = np.log(df.disp) \n",
    "  + left.merge(right, on = 'key')\n",
    "  + left.merge(right, left_an = 'a', right_on = 'b') \n",
    "  + pd.pivot_table(df, values = 'D', index = ['A', 'B'], columns = [ 'C'] , aggfunc = np.sum)\n",
    "  + df.describe()\n",
    "  + df.hist()\n",
    "  + no SQL support, there are third party libraries which support SQL like pandasql (open-source) and yhat (commercial) though \n",
    "\n",
    "\n",
    "  \n",
    "PySpark:\n",
    "  + df = spark.read.csv(path_to_csv_file, header=True, inferSchema=True)\n",
    "  + df.show()\n",
    "  + df.show (10)\n",
    "  + df.columns \n",
    "  + df.dtypes\n",
    "  + df.toDF('a', 'b') because Spark dataframes are immutable we cannot just make assignments and instead we have to actually craete a new dataframe with those names\n",
    "  + df.withColumnRenamed('old', 'new') \n",
    "  + df.drop ('mpg') we don't have an axis concept here unlike in Pandas and we don't have index and so the only thing we can do is to drop columns \n",
    "  + df[df.mpg < 20] \n",
    "  + df.withColumn('gpm', 1 / df.mpg) again because of immutability we cannot make just assignment. Devision by zero in Pandas gives infinity but in Spark gives Null \n",
    "  + df.fillna(0) much less options compared to Pandas \n",
    "  + df.groupby(['cyl', 'gear']).agg({'mpg': 'mean', 'disp': 'min'}) \n",
    "  + import pyspark.sql.functions as F --> this keeps compute in the JVM and not running any python in the executor meaning it is faster \n",
    "    + df.withColumn ('logdisp', F.log(df.disp))\n",
    "  + left.join(right, on = 'key')\n",
    "  + left.join(right, left.a == right.b) \n",
    "  + df.groupBy(\"A\", \"B \").pivot(\"C\").sum(\"D\")\n",
    "  + df.describe().show() (only count, mean, stddev, min, max, NO Quartiles) to get quartiles we need more code using built-in function called percentile_approx \n",
    "  + df.sample(False, 0.1).toPandas().hist()\n",
    "  + great SQL support \n",
    "    + df.createOrReplaceTempView('foo')\n",
    "    + df2 = spark.sql('select * from foo')\n",
    "  \n",
    "  PySpark best practices:\n",
    "  + make sure to use pyspark.sql.functions and other built-in functions\n",
    "  + use the same version of python and packages on cluster as driver \n",
    "  + learn about SSH port forwarding \n",
    "  + MLlib for ML at scale, which is equivalent to scikit-learn but in PySpark  \n",
    "  + don't iterate through rows \n",
    "  + do df.limit(5).toPandas() NOT df.toPandas().head() \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb36b4",
   "metadata": {},
   "source": [
    "# Course Notes\n",
    "\n",
    "sources: Spark and Python for Big Data class by Jose Portilla on udemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6056e",
   "metadata": {},
   "source": [
    "1. Spark runs programs up to 100x faster than Hadoop MapReduce in memory \n",
    "\n",
    "2. What is big datta?\n",
    "Data that can fit on a local computer, in the scale of 0-32 GB depending on RAM, is not big data. \n",
    "\n",
    "3. But what can we do if we have a larger set of data?\n",
    "\n",
    "    + **Try using a SQL database to move storage onto hard drive instead of RAM**\n",
    "    \n",
    "    + **Or use a distributed system, that distributes the data to multiple machines/computer. This is where Spark comes to play.**\n",
    "\n",
    "If you're using spark, you're at a point where it no longer makes sense to fit all your data on RAM and it no longer makes sense to fit all your data into a single machine.\n",
    "\n",
    "4. Local vs. distributed system: \n",
    "\n",
    "a local system is probably what you're used to. It's just a single machine, a single computer.It all shares the same ram, the same hard drive. A local process will use the computation resources of a single machine. \n",
    "\n",
    "In a distributed system, you have one main computer, some sort of master node, and you also have data and calculations distributed onto the other computers. A distributed process has access to the computational resources across a number of machines connected through a network. \n",
    "\n",
    "5. Scaling\n",
    "\n",
    "+ After a certain point, it's much easier to scale out to many lower CPU machines in a distributed system, than try to scale up to a single machine with a high CPU\n",
    "+ distributed machines also have the advantage of easily scaling. All you have to do is just add more machines versus a single computer.\n",
    "\n",
    "No matter how nice it is, there's going to be a limit on how much ram or how much storage you can add to a single machine. So in distributed machines, you can just keep adding systems to the network and just get more power\n",
    "\n",
    "6. Fault-tolerance\n",
    "\n",
    "Distributed machines also include fault tolerance, which is really important when you're talking about large data sets. If one machine fails, the whole network can still go on, which you can't do on a local machine if your local machine crashes due to some error in the calculation you just lost all your calculation, all your data and fault tolerance is a fundamental idea where you're going to be replicating your data across multiple machines. So even if one goes down, your calculations and your data still persists and goes on.\n",
    "\n",
    "7. Distributed architecture of Haddop\n",
    "\n",
    "    + Hadoop is a way to distribute very large files across multiple machines.\n",
    "    + It uses the Hadoop Distributed File System (HDFS)\n",
    "    + HDFS allows a user to work with large data sets\n",
    "    + HDFS also duplicates blocks of data for fault tolerance\n",
    "    + It also then uses MapReduce\n",
    "    + MapReduce allows computations on that data\n",
    "\n",
    "So we kind of have two fundamental ideas here:\n",
    "\n",
    "+ Hadoop is distributed file system, which is the way that we get a really large data set distributed across multiple machines.\n",
    "\n",
    "+ And then we have the idea of MapReduce, which allows computations across the distributed data set.\n",
    "\n",
    "8. Distributed storage - HDFS\n",
    "    + HDFS will use blocks of data, with a size of 128 MB by default\n",
    "    + Each of these blocks is replicated 3 times\n",
    "    + The blocks are distributed in a way to support fault tolerance\n",
    "    + Smaller blocks provide more parallelization during processing\n",
    "    + Multiple copies of a block prevent loss of data due to a failure of a node\n",
    "\n",
    "\n",
    "9. MapReduce\n",
    "\n",
    "    + MapReduce is a way of splitting a **computation task** to a distributed set of files (such as HDFS)\n",
    "    + It consists of a Job Tracker and multiple Task Trackers\n",
    "\n",
    "10. What we covered can be thought of in two distinct parts:\n",
    "    + Using HDFS to distribute large data sets\n",
    "    + Using MapReduce to distribute a computational task to a distributed data set\n",
    "\n",
    "Spark improves on the concepts of using distribution\n",
    "\n",
    "11. Spark is a flexible alternative to MapReduce NOT to Hadoop\n",
    "\n",
    "So don't think of spark in the concepts of Hadoop versus Spark, but instead  MapReduce versus Spark.\n",
    "\n",
    "12. Spark vs MapReduce\n",
    "\n",
    "    + MapReduce requires files to be stored in HDFS, Spark does not! Spark can use data stored in a variety of formats\n",
    "    \n",
    "            Cassandra\n",
    "\n",
    "            AWS S3\n",
    "\n",
    "            HDFS\n",
    "\n",
    "            And more\n",
    "\n",
    "    + Spark also can perform operations up to 100x faster than MapReduce\n",
    " \n",
    "13. So how does spark achieve this speed?\n",
    "\n",
    "    + MapReduce writes most data to disk after each map and reduce operation\n",
    "    + Spark keeps most of the data in memory after each transformation\n",
    "    + Spark can spill over to disk if the memory is filled\n",
    "\n",
    "14. Spark RRD \n",
    "\n",
    "At the core of Spark is the idea of a Resilient Distributed Dataset (RDD)\n",
    "Resilient Distributed Dataset (RDD) has 4 main features:\n",
    "\n",
    "+ Distributed Collection of Data\n",
    "+ Fault-tolerant\n",
    "+ Parallel operation - partioned\n",
    "+ Ability to use many data sources\n",
    "\n",
    "RDDs are **immutable, lazily evaluated, and cacheable**\n",
    "\n",
    "There are two types of Spark operations:\n",
    "\n",
    "+ Transformations: Transformations are basically a recipe to follow.\n",
    "+ Actions: Actions actually perform what the recipe says to do and returns something back.\n",
    "\n",
    "15. Transformations vs actions behavior \n",
    "\n",
    "The behavior of transformations versus actions also carries over to the syntax when coding. A lot of times when you write a method call off of a data frame which we're going to be working with with pyspark, you won't see anything as a result until you call an action something like show. And this makes sense because if you're working with a really large data set, you don't want to constantly be calculating all the transformations. Maybe a transformation can be something like take the average or take the count of a particular data, or show me where column X is greater than the number two, etc. like that. But you don't want to actually calculate that every time until you're sure you want to perform it because it's such a huge data set. It's quite a task to calculate everything every time you type something. So that's why everything is separated between transformations and then those calls to action.\n",
    "\n",
    "16. Spark DataFrames are also now the standard way of using Sparkâ€™s Machine Learning Capabilities.\n",
    "\n",
    "17. why Linux? \n",
    "\n",
    "Realistically Spark won't be running on a single machine. That's basically the whole point of spark. Your data is so large that it no longer fits on a single machine and you're going to need to run it on a cluster on a service like Google Cloud or Amazon Web Services. **And these cluster services will pretty much always be on a Linux based system.** They're not running Mac OS or Windows.\n",
    "\n",
    "Employers and the real world is really focused on Linux when it comes to spark, which makes sense because if you're running it on a cluster, it's going to be on a Linux based system.\n",
    "\n",
    "18. findspark library\n",
    "\n",
    "I need to install find Spark library to not be needing to change directory every time and instead, being able to import Spark from any directory so I don't have to worry about changing directory to that spark home directory.\n",
    "\n",
    "**pip3 install findspark **\n",
    "\n",
    "then \n",
    "\n",
    "    import findspark\n",
    "    findspark.init('/home/danial/spark-3.3.2-bin-hadoop3')\n",
    "    import pyspark \n",
    "\n",
    "19. Intro to Spark DF \n",
    "\n",
    "Spark in its early days began with something known as the **RDD syntax**, which was a little ugly and a bit tricky to learn. Fortunately, now, Spark 2.0 and higher has shifted towards a **data frame syntax**, which is much cleaner and easier to work with. And this **data frame syntax looks really similar across all the APIs**, which is nice. Meaning if you've already done a course in something like Scala and Spark Learning Python and Spark data frames is really easy. A lot of that stuff looks extremely similar.\n",
    "\n",
    "\n",
    "20. How to define Schema\n",
    "\n",
    "Often if you're not dealing with data that's really nice, or maybe from a particular source, you need to actually clarify what the schema is. So in order to do certain operations, the schema has to be correct. It has to know what columns are strings, what columns are integers, etc..\n",
    "\n",
    "        from pyspark.sql.types import StructField,StringType,IntegerType,StructType\n",
    "        \n",
    "        data_schema = [StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)]\n",
    "\n",
    "        final_struc = StructType(fields=data_schema)\n",
    "\n",
    "        df = spark.read.json('people.json', schema=final_struc)\n",
    "        \n",
    "21. How to grab data from a spark DataFrame\n",
    "**df['age']** gives me back the column object but if I actually want to get a data frame with that singular column so that I can see the results I use the select method:\n",
    "\n",
    "**df.select('age').show()**\n",
    "\n",
    "So the main differences between these two methods is the fact that one of them, the first one, is returning back a column, while the second one is returning a data frame that contains a single column, so we have a lot more methods and attributes we can call off of that.\n",
    "\n",
    "You get a lot more flexibility with a data frame of a single column versus just a column So a lot of times we use select instead of just grabbing that column object.\n",
    "\n",
    "df.head(2) gives me back a list of row object.\n",
    "\n",
    "And the reason there are so many specialized objects, such as a column object or a row object, is because of **Spark's ability to read from a distributed data source and then map that out to distributed computing.**\n",
    "\n",
    "to select multiple columns:\n",
    "\n",
    "**df.select(['age', 'name'])**\n",
    "\n",
    "22. Adding new column using **withColumn method**\n",
    "\n",
    "withColumn method basically returns a new dataframe by adding in a column or replacing an existing column. This is not an in place operation and we would have to save this to a new dataframe.\n",
    "\n",
    "23. Renaming a column using withColumnRenamed method\n",
    "\n",
    "**df.withColumnRenamed('old_col_name', 'new_col_name')**\n",
    "\n",
    "24. Using pure SQL to directly deal and interact with the spark data frame\n",
    "\n",
    "First I need to register the DataFrame as a SQL temporary view then I can pass in direct SQL queries:\n",
    "\n",
    "**df.crerateOrReplaceTempView('people')**\n",
    "\n",
    "**results = spark.sql(\"SELECT * FROM people WHERE age=30\")**\n",
    "\n",
    "**results.show()**\n",
    "\n",
    "What's really awesome is if you already have a lot of SQL knowledge, you can leverage that with spark SQL and you can do complicated operations really quickly in case you happen to forget some of the more basic spark data frame operations.\n",
    "\n",
    "25. How to filter data when you grabed it\n",
    "\n",
    "A large part of working with data frames is the ability to quickly filter out data based on conditions. Spark data frames are built on top of that Spark SQL platform, which means, as we previously discussed, if you already know SQL, you could quickly and easily grab that data using SQL commands. However, we're really going to be using the data frame methods as our focus for the course. but here is what it looks like using SQL:\n",
    "\n",
    "**df.filter(\"Close < 500\").select([\"Open\", \"Close\"]).show()**\n",
    "\n",
    "but the above operation using normal python comparison operations is like:\n",
    "\n",
    "**df.filter(df[\"Close\"] < 500).select([\"Open\", \"Close\"]).show()**\n",
    "\n",
    "The key things to keep in mind here is that I'm using dot filter and then passing in the column, some comparison operator and then the value.\n",
    "\n",
    "**filtering based on multiple conditions**:\n",
    "\n",
    "**df.filter((df[\"Close\"] < 500) & ~(df[\"Open\"] > 200)).show()**\n",
    "\n",
    "26. collect method\n",
    "\n",
    "And when you're working in the real world with data, a lot of times you're going to want to collect stuff so you can actually work with that variable later on. Often in this course, we'll just using we'll just be using **show** so you can actually see stuff, but we don't really need to collect it. But **in real life you'll probably be collecting more often than showing.**\n",
    "\n",
    "**result = df.filter(df[\"Low\"] == 197.16).collect()** \n",
    "\n",
    "**row = result[0]**\n",
    "\n",
    "**row.asDict()**\n",
    "\n",
    "27. Groupby and Aggregate Operations \n",
    "\n",
    "        import findspark\n",
    "        findspark.init(\"home/danial/spark-3.3.2-bin-hadoop3\")\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"aggs\").getOrCreate()\n",
    "        df = spark.read.csv(path, inferSchema=True, heade=True)\n",
    "\n",
    "        df.groupBy(\"Company\").mean().show()\n",
    "\n",
    "    We can import functions from pyspark.sql.functions, then what I can do with these functions is combine them with a **select call**:\n",
    "\n",
    "        from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "        df.select(avg(\"Sales\")).show()\n",
    "        df.select(avg(\"Sales\").alias(\"Average Sales\")).show()\n",
    "\n",
    "        from pyspark.sql.functions import format_number\n",
    "\n",
    "        df.select(stddev(\"Sales\").alias(\"std\")).select(format_number(\"std\", 2).alias(\"std\")).show() \n",
    "        \n",
    "        df.orderBy(\"Sales\").show() # in ascending order \n",
    "        df.orderBy(df[\"Sales\"].desc()).show() # in descending order \n",
    "        \n",
    "28. Missing values \n",
    "\n",
    "        import findspark\n",
    "        findspark.init(\"home/danial/spark-3.3.2-bin-hadoop3\")\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"miss\").getOrCreate()\n",
    "        df = spark.read.csv(path, inferSchema=True, heade=True)\n",
    "\n",
    "        df.na.drop().show()\n",
    "        df.na.drop(how='any').show() # how is defaulted to any \n",
    "        df.na.drop(how='all').show()\n",
    "\n",
    "        df.na.drop(thresh=2).show()\n",
    "        df.na.drop(subset=['Sales']).show()\n",
    "\n",
    "       the best practice is to make it clear which column you want to fill null in (spark is smart enought to figure it out on her/his own though):\n",
    "       \n",
    "        df.na.fill('No Name', subset=['Name']).show() \n",
    "\n",
    "        from pyspark.sql.functions import mean \n",
    "        mean_val = df.select(mean(df['Sales'])).collect()\n",
    "        mean_sales = mean_val[0][0]\n",
    "        df.na.fill(mean_sales, ['Sales']).show()\n",
    "            \n",
    "29. Dates and Timestamps\n",
    "\n",
    "Basically whenever I want to use any pyspark.sql.functions, first I need to do **df.select** then I just call the function on whatever column I want so I have to pass in the actual column meaning I have to use **bracket notation** like:\n",
    "\n",
    "    from pyspark.sql.functions import dayofmonth, year, format_number \n",
    "    df.select(dayofmonth(df[\"Date\"])).show()\n",
    "    \n",
    "    \n",
    "    newdf  = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "    result = newdf.groupBy(\"Year\").mean().select([\"Year\", \"avg(Close)\"]) \n",
    "    result.select([\"Year\", format_number(\"avg(Close)\", 2).alias(\"Average Closing Price\")]).show()\n",
    "\n",
    "\n",
    "30. Intro to ML with MLlib \n",
    "\n",
    "So one of the main quirks when dealing with the MLlib is that you need to format your data. So eventually just has one or two columns. And if you're using a supervised learning algorithm, the two columns are going to be features and labels, for unsupervised it's just a features column.\n",
    "\n",
    "Basically what that means is if you have a data set with a ton of feature columns, you eventually need to **condense those all down to just a singular column** where each entry in that singular column, so the rows, is actually just an array consisting of all those old entries.\n",
    "\n",
    "So overall, this requires a little more data processing work than some other machine learning libraries. But the big upside and the whole reason for all this data processing work is that **that exact same syntax will work with distributed data**. So if you have a huge data set, you don't need to learn a new syntax for it. So that's no small feat for what's actually going on under the hood with Python and spark. It just requires you to put in a little more work with data processing using different **vector indexers**.\n",
    "\n",
    "31. Linear regression using MLlib\n",
    "\n",
    "        import findspark\n",
    "        findspark.init(\"home/danial/spark-3.3.2-bin-hadoop3\")\n",
    "        \n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.appName(\"lrm\").getOrCreate()\n",
    "        \n",
    "        \n",
    "        from pyspark.ml.regression import LinearRegression \n",
    "        path = 'path_to_data'\n",
    "        data = spark.read.format('libsvm').load(path)\n",
    "        \n",
    "        train_set, test_set = data.randomSplit([0.7, 0.3]) \n",
    "        \n",
    "        lr = LinearRegression(featuresCol='features', labelCol='label', predictionCol='predictions')\n",
    "        lrModel = lr.fit(train_set)\n",
    "        \n",
    "        lrModel.coefficients\n",
    "        lrModel.intercept\n",
    "        \n",
    "        summary = lrModel.summary\n",
    "        summary.r2\n",
    "        summary.rootMeanSquaredError \n",
    "        \n",
    "        test_results = lrModel.evaluate(test_set)\n",
    "        \n",
    "        test_results.residuals.show()\n",
    "        test_results.rootMeanSquaredError\n",
    "        \n",
    "        # pretending to have unlabelled data for model deployment \n",
    "        \n",
    "        unlabelled_data = test_set.select('features')\n",
    "        predictions = lrModel.transform(unlabelled_data)\n",
    "        predictions.show()\n",
    "\n",
    "32. Data transformation to prepare features for MLlib\n",
    "\n",
    "Usually what we end up doing is combining the various feature columns in a realistic data set into a single features column using the data transformations:\n",
    "\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numerical_features, outputCol='features')\n",
    "    output = assembler.transform(df)\n",
    "    output.printSchema()\n",
    "    final_data = output.select('features', 'Yearly Amount Spent')\n",
    "    \n",
    "    train_set, test_set = final_data.randomSplit([0.7, 0.3]) # I need to transform all the data before splitting it into training/testing.\n",
    "\n",
    "    lr = LinearRegression(labelCol='Yearly Amount Spent')\n",
    "    lr_model = lr.fit(train_set)\n",
    "    test_results = lr_model.evaluate(test_set)\n",
    "\n",
    "    test_results.residuals.show()\n",
    "    test_results.rootMeanSquaredError\n",
    "    test_results.r2\n",
    "    final_data.describe().show()\n",
    "    \n",
    "    unlabelled_data = test_set.select('features') # let's pretend we have unlabelled data on which I would like to deploy my model\n",
    "    predictions = lr_model.transform(unlabelled_data)\n",
    "    predictions.show()\n",
    "    \n",
    "33. How to convert categorical columns into numerical usint **StringIndexer**\n",
    "\n",
    "In Apache Spark, StringIndexer is a **feature transformer that converts a categorical column of strings into a column of numerical indices.** It assigns a unique numerical value to each distinct string in the column, based on the frequency of occurrence. The most frequent string is assigned an index of 0, the second most frequent string is assigned an index of 1, and so on.\n",
    "\n",
    "The StringIndexer takes the following arguments:\n",
    "\n",
    "inputCol: The name of the input column of string type to be indexed.\n",
    "outputCol: The name of the output column of numerical indices.\n",
    "\n",
    "example:\n",
    "\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    indexer = StringIndexer(inputCol='Cruise_line', outputCol='Cruise_line_Index')\n",
    "    indexed = indexer.fit(data).transform(data)\n",
    "\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    num_col = ['Age',\n",
    "             'Tonnage',\n",
    "             'passengers',\n",
    "             'length',\n",
    "             'cabins',\n",
    "             'passenger_density',\n",
    "             'Cruise_line_Index']\n",
    "    assembler = VectorAssembler(inputCols=num_col, outputCol='features')\n",
    "    output = assembler.transform(indexed)\n",
    "\n",
    "    final_data = output.select('features', 'crew')\n",
    "    train_set, test_set = final_data.randomSplit([0.7, 0.3])\n",
    "    lr = LinearRegression(labelCol='crew')\n",
    "    lr_model = lr.fit(train_set)\n",
    "    test_results = lr_model.evaluate(test_set)\n",
    "    test_results.residuals.show()\n",
    "    test_results.r2\n",
    "    test_results.rootMeanSquaredError\n",
    "    test_results.meanAbsoluteErrortest_results.meanSquaredError\n",
    "\n",
    "    from pyspark.sql.functions import corr\n",
    "    data.select(corr('crew','passengers')).show()\n",
    "    data.select(corr('crew','cabins')).show()\n",
    "\n",
    "34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7f3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
